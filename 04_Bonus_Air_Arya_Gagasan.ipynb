{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVsAIlNUF0NS3gtmrAVhnq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryagagas/machine-learning-and-deep-learning-assignment/blob/main/04_Bonus_Air_Arya_Gagasan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare 3 configurations for the activation function. Show and explain your performance result."
      ],
      "metadata": {
        "id": "q_1ZCnb6AyRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries and Data Preparation"
      ],
      "metadata": {
        "id": "QtbBkwVXAoz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "# Download and load MNIST dataset\n",
        "train_data = torchvision.datasets.MNIST(root='data', train=True, transform=ToTensor(), download=True)\n",
        "test_data = torchvision.datasets.MNIST(root='data', train=False, transform=ToTensor(), download=True)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNgHuR_D1JON",
        "outputId": "405e9a22-3a1b-45d0-c20f-920c44635d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 24934261.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 3696768.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28555172.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11651699.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluating the Model"
      ],
      "metadata": {
        "id": "ixNyc6frBl56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to train and evaluate a neural network with a specific activation function\n",
        "def train_and_evaluate(model, activation_function, num_epochs=10):\n",
        "    # Define hyperparameters\n",
        "    learning_rate = 0.01\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "            for images, labels in test_loader:\n",
        "                outputs = model(images)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            acc = accuracy_score(all_labels, all_preds)\n",
        "            f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "            precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "            recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}: Accuracy={acc:.4f}, F1 Score={f1:.4f}, Precision={precision:.4f}, Recall={recall:.4f}')\n"
      ],
      "metadata": {
        "id": "64Hb7v4aBdsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Neural Network Model"
      ],
      "metadata": {
        "id": "4XUjrnwVBsvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model Configuration\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, activation_function):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.activation = activation_function\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "WwlD1bQSBqq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create models with different activation functions"
      ],
      "metadata": {
        "id": "Jme7WCRHB33Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activation_functions = [nn.ReLU(), nn.Sigmoid(), nn.Tanh()]\n",
        "\n",
        "for activation_function in activation_functions:\n",
        "    model = SimpleModel(activation_function)\n",
        "    print(f\"Model with {activation_function.__class__.__name__} Activation:\")\n",
        "    train_and_evaluate(model, activation_function)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6vbf4PkBv9C",
        "outputId": "e048a2f7-31c0-4f24-a657-fe649ca66728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with ReLU Activation:\n",
            "Epoch 1: Accuracy=0.8112, F1 Score=0.7996, Precision=0.8204, Recall=0.8050\n",
            "Epoch 2: Accuracy=0.8641, F1 Score=0.8607, Precision=0.8631, Recall=0.8612\n",
            "Epoch 3: Accuracy=0.8837, F1 Score=0.8814, Precision=0.8822, Recall=0.8816\n",
            "Epoch 4: Accuracy=0.8935, F1 Score=0.8915, Precision=0.8920, Recall=0.8918\n",
            "Epoch 5: Accuracy=0.8987, F1 Score=0.8970, Precision=0.8975, Recall=0.8971\n",
            "Epoch 6: Accuracy=0.9022, F1 Score=0.9006, Precision=0.9008, Recall=0.9007\n",
            "Epoch 7: Accuracy=0.9047, F1 Score=0.9031, Precision=0.9034, Recall=0.9032\n",
            "Epoch 8: Accuracy=0.9098, F1 Score=0.9083, Precision=0.9088, Recall=0.9085\n",
            "Epoch 9: Accuracy=0.9110, F1 Score=0.9095, Precision=0.9103, Recall=0.9095\n",
            "Epoch 10: Accuracy=0.9144, F1 Score=0.9130, Precision=0.9134, Recall=0.9131\n",
            "\n",
            "\n",
            "Model with Sigmoid Activation:\n",
            "Epoch 1: Accuracy=0.4982, F1 Score=0.4325, Precision=0.6299, Recall=0.4861\n",
            "Epoch 2: Accuracy=0.5900, F1 Score=0.5279, Precision=0.7263, Recall=0.5781\n",
            "Epoch 3: Accuracy=0.7172, F1 Score=0.6794, Precision=0.7711, Recall=0.7072\n",
            "Epoch 4: Accuracy=0.7769, F1 Score=0.7650, Precision=0.7967, Recall=0.7705\n",
            "Epoch 5: Accuracy=0.7944, F1 Score=0.7822, Precision=0.8069, Recall=0.7886\n",
            "Epoch 6: Accuracy=0.8188, F1 Score=0.8119, Precision=0.8235, Recall=0.8142\n",
            "Epoch 7: Accuracy=0.8365, F1 Score=0.8323, Precision=0.8374, Recall=0.8331\n",
            "Epoch 8: Accuracy=0.8493, F1 Score=0.8454, Precision=0.8490, Recall=0.8461\n",
            "Epoch 9: Accuracy=0.8583, F1 Score=0.8555, Precision=0.8572, Recall=0.8559\n",
            "Epoch 10: Accuracy=0.8644, F1 Score=0.8618, Precision=0.8632, Recall=0.8620\n",
            "\n",
            "\n",
            "Model with Tanh Activation:\n",
            "Epoch 1: Accuracy=0.8354, F1 Score=0.8291, Precision=0.8397, Recall=0.8308\n",
            "Epoch 2: Accuracy=0.8723, F1 Score=0.8693, Precision=0.8716, Recall=0.8696\n",
            "Epoch 3: Accuracy=0.8868, F1 Score=0.8847, Precision=0.8855, Recall=0.8850\n",
            "Epoch 4: Accuracy=0.8930, F1 Score=0.8911, Precision=0.8915, Recall=0.8914\n",
            "Epoch 5: Accuracy=0.8980, F1 Score=0.8962, Precision=0.8967, Recall=0.8964\n",
            "Epoch 6: Accuracy=0.9002, F1 Score=0.8986, Precision=0.8988, Recall=0.8987\n",
            "Epoch 7: Accuracy=0.9031, F1 Score=0.9016, Precision=0.9018, Recall=0.9017\n",
            "Epoch 8: Accuracy=0.9063, F1 Score=0.9049, Precision=0.9051, Recall=0.9051\n",
            "Epoch 9: Accuracy=0.9095, F1 Score=0.9081, Precision=0.9085, Recall=0.9082\n",
            "Epoch 10: Accuracy=0.9117, F1 Score=0.9104, Precision=0.9107, Recall=0.9106\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show and explain the performance result"
      ],
      "metadata": {
        "id": "9G7kw57JCtBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided performance results are for three different configurations of a neural network model with different activation functions: ReLU, Sigmoid, and Tanh. Let's analyze and explain the performance results for each configuration:\n",
        "\n",
        "<br>**Model with ReLU Activation:**\n",
        "- ReLU (Rectified Linear Unit) is known for its ability to handle the vanishing gradient problem and accelerate the convergence of training.\n",
        "- As seen in the results, the accuracy increases consistently with each epoch, reaching 91.44% accuracy by the 10th epoch.\n",
        "- F1 Score, Precision, and Recall all show steady improvement over epochs, indicating that the model is learning effectively.\n",
        "- ReLU is a popular choice for many deep learning tasks due to its effectiveness.\n",
        "\n",
        "<br>**Model with Sigmoid Activation:**\n",
        "- Sigmoid activation outputs values between 0 and 1, which can be interpreted as probabilities.\n",
        "- The accuracy starts very low at 49.82% and gradually improves to 86.44% by the 10th epoch. However, it lags behind ReLU.\n",
        "- F1 Score, Precision, and Recall also improve but at a slower rate compared to ReLU.\n",
        "- Sigmoid tends to suffer from the vanishing gradient problem, which can make training slower and less effective for deep networks.\n",
        "\n",
        "<br>**Model with Tanh Activation:**\n",
        "- Tanh activation is similar to sigmoid but has values between -1 and 1.\n",
        "- The accuracy starts at 83.54% and reaches 91.17% by the 10th epoch, which is better than the Sigmoid model but slightly below the ReLU model.\n",
        "- F1 Score, Precision, and Recall all show steady improvement over epochs, similar to ReLU.\n",
        "- Tanh can be useful when you want values centered around zero, and it helps mitigate the vanishing gradient problem better than Sigmoid.\n",
        "\n",
        "<br>In summary, the ReLU activation function performs the best among the three configurations, achieving the highest accuracy, F1 Score, Precision, and Recall. It converges faster and more effectively during training. Tanh also performs well, while Sigmoid lags behind in terms of accuracy and training speed. The choice of activation function should depend on the specific problem and architecture, but ReLU is often a good default choice for many scenarios."
      ],
      "metadata": {
        "id": "GR7Etp-oC3y2"
      }
    }
  ]
}